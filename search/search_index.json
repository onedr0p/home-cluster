{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Cluster \u00b6 This repository is my home Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a Github issue or join the k8s@home Discord if you have any questions. This repository is built off the k8s-at-home/template-cluster-k3s repository. Cluster setup \u00b6 My cluster is k3s provisioned overtop Ubuntu 21.04 using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes. See my ansible directory for my playbooks and roles. Cluster components \u00b6 calico : For internal cluster networking using BGP configured on Opnsense. rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository. external-dns : Creates DNS entries in a separate coredns deployment which is backed by my clusters etcd deployment. cert-manager : Configured to create TLS certs for all ingress services automatically using LetsEncrypt. kube-vip : HA solution for Kubernetes control plane Kasten : Data backup and recovery Repository structure \u00b6 The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. base directory is the entrypoint to Flux crds directory contains custom resource definitions (CRDs) that need to exist globally in your cluster before anything else exists core directory (depends on crds ) are important infrastructure applications (grouped by namespace) that should never be pruned by Flux apps directory (depends on core ) is where your common applications (grouped by namespace) could be placed, Flux will prune resources here if they are not tracked by Git anymore ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u2514\u2500\u2500 ./crds Automate all the things! \u00b6 Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date Networking \u00b6 Currently when using BGP on Opnsense, services do not get properly load balanced. This is due to Opnsense not supporting multipath in the BSD kernel. In my network Calico is configured with BGP on my Opnsense router. With BGP enabled, I advertise a load balancer using externalIPs on my Kubernetes services. Name CIDR Management 192.168.1.0/24 Servers 192.168.42.0/24 k8s external services (BGP) 192.168.69.0/24 k8s pods 10.69.0.0/16 k8s services 10.96.0.0/16 DNS \u00b6 To prefix this, I should mention that I only use one domain name for internal and externally facing applications. Also this is the most complicated thing to explain but I will try to sum it up. On Opnsense under Services: Unbound DNS: Overrides I have a Domain Override set to my domain with the address pointing to my in-cluster-non-cluster service CoreDNS load balancer IP. This allows me to use Split-horizon DNS . external-dns reads my clusters Ingress 's and inserts DNS records containing the sub-domain and load balancer IP (of traefik) into the in-cluster-non-cluster service CoreDNS service and into Cloudflare depending on if an annotation is present on the ingress. See the diagram below for a visual representation. Hardware \u00b6 Device Count OS Disk Size Data Disk Size Ram Purpose Intel NUC8i3BEK 3 256GB NVMe N/A 16GB k3s Masters (embedded etcd) Intel NUC8i5BEH 1 240GB SSD 1TB NVMe (rook-ceph) 32GB k3s Workers Intel NUC8i7BEH 2 240GB SSD 1TB NVMe (rook-ceph) 32GB k3s Workers TrueNAS SCALE (custom) 1 120GB SSD 8x12TB RAIDz2 64GB Shared file storage Tools \u00b6 Tool Purpose direnv Sets environment variable based on present working directory go-task Alternative to makefiles, who honestly likes that? pre-commit Enforce code consistency and verifies no secrets are pushed stern Tail logs in Kubernetes Thanks \u00b6 A lot of inspiration for my cluster came from the people that have shared their clusters over at awesome-home-kubernetes","title":"Introduction"},{"location":"#home-cluster","text":"This repository is my home Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a Github issue or join the k8s@home Discord if you have any questions. This repository is built off the k8s-at-home/template-cluster-k3s repository.","title":"Home Cluster"},{"location":"#cluster-setup","text":"My cluster is k3s provisioned overtop Ubuntu 21.04 using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes. See my ansible directory for my playbooks and roles.","title":"Cluster setup"},{"location":"#cluster-components","text":"calico : For internal cluster networking using BGP configured on Opnsense. rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository. external-dns : Creates DNS entries in a separate coredns deployment which is backed by my clusters etcd deployment. cert-manager : Configured to create TLS certs for all ingress services automatically using LetsEncrypt. kube-vip : HA solution for Kubernetes control plane Kasten : Data backup and recovery","title":"Cluster components"},{"location":"#repository-structure","text":"The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. base directory is the entrypoint to Flux crds directory contains custom resource definitions (CRDs) that need to exist globally in your cluster before anything else exists core directory (depends on crds ) are important infrastructure applications (grouped by namespace) that should never be pruned by Flux apps directory (depends on core ) is where your common applications (grouped by namespace) could be placed, Flux will prune resources here if they are not tracked by Git anymore ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u2514\u2500\u2500 ./crds","title":"Repository structure"},{"location":"#automate-all-the-things","text":"Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date","title":"Automate all the things!"},{"location":"#networking","text":"Currently when using BGP on Opnsense, services do not get properly load balanced. This is due to Opnsense not supporting multipath in the BSD kernel. In my network Calico is configured with BGP on my Opnsense router. With BGP enabled, I advertise a load balancer using externalIPs on my Kubernetes services. Name CIDR Management 192.168.1.0/24 Servers 192.168.42.0/24 k8s external services (BGP) 192.168.69.0/24 k8s pods 10.69.0.0/16 k8s services 10.96.0.0/16","title":"Networking"},{"location":"#dns","text":"To prefix this, I should mention that I only use one domain name for internal and externally facing applications. Also this is the most complicated thing to explain but I will try to sum it up. On Opnsense under Services: Unbound DNS: Overrides I have a Domain Override set to my domain with the address pointing to my in-cluster-non-cluster service CoreDNS load balancer IP. This allows me to use Split-horizon DNS . external-dns reads my clusters Ingress 's and inserts DNS records containing the sub-domain and load balancer IP (of traefik) into the in-cluster-non-cluster service CoreDNS service and into Cloudflare depending on if an annotation is present on the ingress. See the diagram below for a visual representation.","title":"DNS"},{"location":"#hardware","text":"Device Count OS Disk Size Data Disk Size Ram Purpose Intel NUC8i3BEK 3 256GB NVMe N/A 16GB k3s Masters (embedded etcd) Intel NUC8i5BEH 1 240GB SSD 1TB NVMe (rook-ceph) 32GB k3s Workers Intel NUC8i7BEH 2 240GB SSD 1TB NVMe (rook-ceph) 32GB k3s Workers TrueNAS SCALE (custom) 1 120GB SSD 8x12TB RAIDz2 64GB Shared file storage","title":"Hardware"},{"location":"#tools","text":"Tool Purpose direnv Sets environment variable based on present working directory go-task Alternative to makefiles, who honestly likes that? pre-commit Enforce code consistency and verifies no secrets are pushed stern Tail logs in Kubernetes","title":"Tools"},{"location":"#thanks","text":"A lot of inspiration for my cluster came from the people that have shared their clusters over at awesome-home-kubernetes","title":"Thanks"},{"location":"installation/bootstrap-applications/","text":"Bootstrapping Applications \u00b6 The Kubernetes @Home community has created a wonderful Github template for bootstrapping a cluster for flux this can be viewed here . Bootstrapping Flux \u00b6 Create or locate cluster GPG key \u00b6 export GPG_TTY = $( tty ) gpg --list-secret-keys \"Home cluster (Flux) <email>\" export FLUX_KEY_FP = ABCDEFGHIJKLMNOPQRSTUVWXYZ Verify cluster is ready for Flux \u00b6 flux --kubeconfig = ./kubeconfig check --pre Pre-create the flux-system namespace \u00b6 kubectl --kubeconfig = ./kubeconfig create namespace flux-system --dry-run = client -o yaml | kubectl --kubeconfig = ./kubeconfig apply -f - Add the Flux GPG key in-order for Flux to decrypt SOPS secrets \u00b6 gpg --export-secret-keys --armor \" ${ FLUX_KEY_FP } \" | kubectl --kubeconfig = ./kubeconfig create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin Install Flux \u00b6 Due to race conditions with the Flux CRDs you will have to run the below command twice. There should be no errors on this second run. kubectl --kubeconfig = ./kubeconfig apply --kustomize = ./cluster/base/flux-system at this point after reconciliation Flux state should be restored.","title":"Bootstrap Applications"},{"location":"installation/bootstrap-applications/#bootstrapping-applications","text":"The Kubernetes @Home community has created a wonderful Github template for bootstrapping a cluster for flux this can be viewed here .","title":"Bootstrapping Applications"},{"location":"installation/bootstrap-applications/#bootstrapping-flux","text":"","title":"Bootstrapping Flux"},{"location":"installation/bootstrap-applications/#create-or-locate-cluster-gpg-key","text":"export GPG_TTY = $( tty ) gpg --list-secret-keys \"Home cluster (Flux) <email>\" export FLUX_KEY_FP = ABCDEFGHIJKLMNOPQRSTUVWXYZ","title":"Create or locate cluster GPG key"},{"location":"installation/bootstrap-applications/#verify-cluster-is-ready-for-flux","text":"flux --kubeconfig = ./kubeconfig check --pre","title":"Verify cluster is ready for Flux"},{"location":"installation/bootstrap-applications/#pre-create-the-flux-system-namespace","text":"kubectl --kubeconfig = ./kubeconfig create namespace flux-system --dry-run = client -o yaml | kubectl --kubeconfig = ./kubeconfig apply -f -","title":"Pre-create the flux-system namespace"},{"location":"installation/bootstrap-applications/#add-the-flux-gpg-key-in-order-for-flux-to-decrypt-sops-secrets","text":"gpg --export-secret-keys --armor \" ${ FLUX_KEY_FP } \" | kubectl --kubeconfig = ./kubeconfig create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin","title":"Add the Flux GPG key in-order for Flux to decrypt SOPS secrets"},{"location":"installation/bootstrap-applications/#install-flux","text":"Due to race conditions with the Flux CRDs you will have to run the below command twice. There should be no errors on this second run. kubectl --kubeconfig = ./kubeconfig apply --kustomize = ./cluster/base/flux-system at this point after reconciliation Flux state should be restored.","title":"Install Flux"},{"location":"installation/installing-kubernetes/","text":"Installing Kubernetes \u00b6 Update Ansible inventory configuration and run the k3s-install playbook ansible-playbook -i ansible/inventory/home-cluster/hosts.yml ansible/playbooks/kubernetes/k3s-prepare.yml","title":"Installing Kubernetes"},{"location":"installation/installing-kubernetes/#installing-kubernetes","text":"Update Ansible inventory configuration and run the k3s-install playbook ansible-playbook -i ansible/inventory/home-cluster/hosts.yml ansible/playbooks/kubernetes/k3s-prepare.yml","title":"Installing Kubernetes"},{"location":"installation/preparing-nodes/","text":"Preparing Nodes \u00b6 Install Ubuntu \u00b6 Download Ubuntu Server 21.04 ISO and flash it to a USB drive, boot the device from the USB drive and install Ubuntu Copy over SSH key from the machine running Ansible \u00b6 ssh-copy-id ubuntu@192.168.42.10 Configure static IPs \u00b6 Set a static IP on your nodes or you may run into issues with Alpine containers # /etc/netplan/00-installer-config.yaml network : ethernets : eno1 : addresses : - 192.168.42.10/24 gateway4 : 192.168.42.1 nameservers : addresses : - 192.168.1.1 search : [] version : 2 Prepare Ubuntu for k8s \u00b6 Update Ansible inventory configuration and run the ubuntu-prepare playbook ansible-playbook -i ansible/inventory/home-cluster/hosts.yml ansible/playbooks/kubernetes/ubuntu-prepare.yml","title":"Preparing Nodes"},{"location":"installation/preparing-nodes/#preparing-nodes","text":"","title":"Preparing Nodes"},{"location":"installation/preparing-nodes/#install-ubuntu","text":"Download Ubuntu Server 21.04 ISO and flash it to a USB drive, boot the device from the USB drive and install Ubuntu","title":"Install Ubuntu"},{"location":"installation/preparing-nodes/#copy-over-ssh-key-from-the-machine-running-ansible","text":"ssh-copy-id ubuntu@192.168.42.10","title":"Copy over SSH key from the machine running Ansible"},{"location":"installation/preparing-nodes/#configure-static-ips","text":"Set a static IP on your nodes or you may run into issues with Alpine containers # /etc/netplan/00-installer-config.yaml network : ethernets : eno1 : addresses : - 192.168.42.10/24 gateway4 : 192.168.42.1 nameservers : addresses : - 192.168.1.1 search : [] version : 2","title":"Configure static IPs"},{"location":"installation/preparing-nodes/#prepare-ubuntu-for-k8s","text":"Update Ansible inventory configuration and run the ubuntu-prepare playbook ansible-playbook -i ansible/inventory/home-cluster/hosts.yml ansible/playbooks/kubernetes/ubuntu-prepare.yml","title":"Prepare Ubuntu for k8s"},{"location":"networking/introduction/","text":"Networking \u00b6 Work in progress This document is a work in progress.","title":"Networking"},{"location":"networking/introduction/#networking","text":"Work in progress This document is a work in progress.","title":"Networking"},{"location":"networking/router/","text":"Router \u00b6 Work in progress This document is a work in progress.","title":"Router"},{"location":"networking/router/#router","text":"Work in progress This document is a work in progress.","title":"Router"},{"location":"storage/kasten-data-backup/","text":"Kasten Data Backup \u00b6 Work in progress This document is a work in progress.","title":"Backing up data using Kasten"},{"location":"storage/kasten-data-backup/#kasten-data-backup","text":"Work in progress This document is a work in progress.","title":"Kasten Data Backup"},{"location":"storage/kasten-data-restore/","text":"Kasten Data Restore \u00b6 Recovering from a K10 backup involves the following sequence of actions Create a Kubernetes Secret, k10-dr-secret, using the passphrase provided while enabling DR \u00b6 kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key = <passphrase> Install a fresh K10 instance \u00b6 Ensure that Flux has correctly deployed K10 to it's namespace kasten-io Provide bucket information and credentials for the object storage location \u00b6 Ensure that Flux has correctly deployed the minio storage profile and that it's accessible within K10 Restoring the K10 backup \u00b6 Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job helm install k10-restore kasten/k10restore --namespace = kasten-io \\ --set sourceClusterID = <source-clusterID> \\ --set profile.name = <location-profile-name> Application recovery \u00b6 Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"Restoring data with Kasten"},{"location":"storage/kasten-data-restore/#kasten-data-restore","text":"Recovering from a K10 backup involves the following sequence of actions","title":"Kasten Data Restore"},{"location":"storage/kasten-data-restore/#create-a-kubernetes-secret-k10-dr-secret-using-the-passphrase-provided-while-enabling-dr","text":"kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key = <passphrase>","title":"Create a Kubernetes Secret, k10-dr-secret, using the passphrase provided while enabling DR"},{"location":"storage/kasten-data-restore/#install-a-fresh-k10-instance","text":"Ensure that Flux has correctly deployed K10 to it's namespace kasten-io","title":"Install a fresh K10 instance"},{"location":"storage/kasten-data-restore/#provide-bucket-information-and-credentials-for-the-object-storage-location","text":"Ensure that Flux has correctly deployed the minio storage profile and that it's accessible within K10","title":"Provide bucket information and credentials for the object storage location"},{"location":"storage/kasten-data-restore/#restoring-the-k10-backup","text":"Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job helm install k10-restore kasten/k10restore --namespace = kasten-io \\ --set sourceClusterID = <source-clusterID> \\ --set profile.name = <location-profile-name>","title":"Restoring the K10 backup"},{"location":"storage/kasten-data-restore/#application-recovery","text":"Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"Application recovery"},{"location":"storage/manual-data-backup/","text":"Manual Data Backup \u00b6 Create the toolbox container \u00b6 Ran from your workstation kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192 .168.42.50:/Data /mnt/nfsdata Restore data from a NFS share \u00b6 Ran from your workstation Pause the Flux Helm Release flux suspend hr home-assistant -n home Scale the application down to zero pods kubectl scale deploy/home-assistant --replicas 0 -n home Get the csi-vol-* string kubectl get pv/ ( kubectl get pv | grep home-assistant-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa \\ | xargs -I {} mount {} /mnt/data tar czvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data/ . umount /mnt/data rbd unmap -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa","title":"Backing up data manually"},{"location":"storage/manual-data-backup/#manual-data-backup","text":"","title":"Manual Data Backup"},{"location":"storage/manual-data-backup/#create-the-toolbox-container","text":"Ran from your workstation kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192 .168.42.50:/Data /mnt/nfsdata","title":"Create the toolbox container"},{"location":"storage/manual-data-backup/#restore-data-from-a-nfs-share","text":"Ran from your workstation Pause the Flux Helm Release flux suspend hr home-assistant -n home Scale the application down to zero pods kubectl scale deploy/home-assistant --replicas 0 -n home Get the csi-vol-* string kubectl get pv/ ( kubectl get pv | grep home-assistant-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa \\ | xargs -I {} mount {} /mnt/data tar czvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data/ . umount /mnt/data rbd unmap -p replicapool csi-vol-ebb786c7-9a6f-11eb-ae97-9a71104156fa","title":"Restore data from a NFS share"},{"location":"storage/manual-data-restore/","text":"Manual Data Restore \u00b6 Create the toolbox container \u00b6 Ran from your workstation kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192 .168.42.50:/Data /mnt/nfsdata Restore data from a NFS share \u00b6 Ran from your workstation Apply the PVC kubectl apply -f cluster/apps/home/home-assistant/config-pvc.yaml Get the csi-vol-* string kubectl get pv/ ( kubectl get pv | grep home-assistant-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I {} sh -c 'mkfs.ext4 {}' rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I {} mount {} /mnt/data tar xvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data umount /mnt/data rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212","title":"Restoring data manually"},{"location":"storage/manual-data-restore/#manual-data-restore","text":"","title":"Manual Data Restore"},{"location":"storage/manual-data-restore/#create-the-toolbox-container","text":"Ran from your workstation kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Ran from the rook-ceph-toolbox mkdir -p /mnt/nfsdata mkdir -p /mnt/data mount -t nfs -o \"nfsvers=4.1,hard\" 192 .168.42.50:/Data /mnt/nfsdata","title":"Create the toolbox container"},{"location":"storage/manual-data-restore/#restore-data-from-a-nfs-share","text":"Ran from your workstation Apply the PVC kubectl apply -f cluster/apps/home/home-assistant/config-pvc.yaml Get the csi-vol-* string kubectl get pv/ ( kubectl get pv | grep home-assistant-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Ran from the rook-ceph-toolbox rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I {} sh -c 'mkfs.ext4 {}' rbd map -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 \\ | xargs -I {} mount {} /mnt/data tar xvf /mnt/nfsdata/Backups/home-assistant.tar.gz -C /mnt/data umount /mnt/data rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212 rbd unmap -p replicapool csi-vol-f7a3b0db-d073-11eb-8ec1-4e450ed3a212","title":"Restore data from a NFS share"},{"location":"unsorted/flux/","text":"Flux \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install fluxcd/tap/flux Install the cluster components \u00b6 For full installation guide visit the Flux installation guide Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster set -x GITHUB_TOKEN xyz ; flux bootstrap github \\ --version = v0.12.1 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster/base \\ --personal \\ --private = false \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work Useful commands \u00b6 Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Flux"},{"location":"unsorted/flux/#flux","text":"Work in progress This document is a work in progress.","title":"Flux"},{"location":"unsorted/flux/#install-the-cli-tool","text":"brew install fluxcd/tap/flux","title":"Install the CLI tool"},{"location":"unsorted/flux/#install-the-cluster-components","text":"For full installation guide visit the Flux installation guide Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster set -x GITHUB_TOKEN xyz ; flux bootstrap github \\ --version = v0.12.1 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster/base \\ --personal \\ --private = false \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work","title":"Install the cluster components"},{"location":"unsorted/flux/#useful-commands","text":"Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Useful commands"},{"location":"unsorted/rook-ceph-maintenance/","text":"Rook-Ceph Maintenance \u00b6 Work in progress This document is a work in progress. Accessing volumes \u00b6 Sometimes I am required to access the data in the pvc , below is an example on how I access the pvc data for my zigbee2mqtt deployment. First start by scaling the app deployment to 0 replicas: kubectl scale deploy/zigbee2mqtt --replicas 0 -n home Get the rbd image name for the app: kubectl get pv/ ( kubectl get pv | grep plex-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Exec into the rook-direct-mount toolbox: kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Create a directory to mount the volume to: mkdir -p /mnt/data Mounting a NFS share This can be useful if you want to move data from or to a nfs share Create a directory to mount the nfs share to: mkdir -p /mnt/nfsdata Mount the nfs share: mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192 .168.42.50:/volume1/Data /mnt/nfs List all the rbd block device names: rbd list --pool replicapool Map the rbd block device to a /dev/rbdX device: rbd map -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Mount the /dev/rbdX device: mount /dev/rbdX /mnt/data At this point you'll be able to access the volume data under /mnt/data , you can change files in any way. Backing up or restoring data from a NFS share Restoring data: rm -rf /mnt/data/* tar xvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data chown -R 568 :568 /mnt/data/ Backing up data: tar czvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data/ . When done you can unmount /mnt/data and unmap the rbd device: umount /mnt/data rbd unmap -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Lastly you need to scale the deployment replicas back up to 1: kubectl scale deploy/zigbee2mqtt --replicas 1 -n home Handling crashes \u00b6 Sometimes rook-ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthy do the following... # list all the crashes ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all Helpful links \u00b6 Common issues kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192.168.42.50:/volume1/Data /mnt/nfs kubectl get pv/(kubectl get pv \\ | grep \"tautulli\" \\ | awk -F' ' '{print $1}') -n media -o json \\ | jq -r '.spec.csi.volumeAttributes.imageName' rbd map -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data rm -rf /mnt/data/* tar xvf /mnt/nfs/backups/tautulli.tar.gz -C /mnt/data # chown -R 568:568 /mnt/data/ umount /mnt/data && \\ rbd unmap -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa ceph mgr module enable rook ceph orch set backend rook ceph dashboard set-alertmanager-api-host http://kube-prometheus-stack-alertmanager.monitoring.svc:9093 ceph dashboard set-prometheus-api-host http://kube-prometheus-stack-prometheus.monitoring.svc:9090","title":"Rook-Ceph Maintenance"},{"location":"unsorted/rook-ceph-maintenance/#rook-ceph-maintenance","text":"Work in progress This document is a work in progress.","title":"Rook-Ceph Maintenance"},{"location":"unsorted/rook-ceph-maintenance/#accessing-volumes","text":"Sometimes I am required to access the data in the pvc , below is an example on how I access the pvc data for my zigbee2mqtt deployment. First start by scaling the app deployment to 0 replicas: kubectl scale deploy/zigbee2mqtt --replicas 0 -n home Get the rbd image name for the app: kubectl get pv/ ( kubectl get pv | grep plex-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Exec into the rook-direct-mount toolbox: kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Create a directory to mount the volume to: mkdir -p /mnt/data Mounting a NFS share This can be useful if you want to move data from or to a nfs share Create a directory to mount the nfs share to: mkdir -p /mnt/nfsdata Mount the nfs share: mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192 .168.42.50:/volume1/Data /mnt/nfs List all the rbd block device names: rbd list --pool replicapool Map the rbd block device to a /dev/rbdX device: rbd map -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Mount the /dev/rbdX device: mount /dev/rbdX /mnt/data At this point you'll be able to access the volume data under /mnt/data , you can change files in any way. Backing up or restoring data from a NFS share Restoring data: rm -rf /mnt/data/* tar xvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data chown -R 568 :568 /mnt/data/ Backing up data: tar czvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data/ . When done you can unmount /mnt/data and unmap the rbd device: umount /mnt/data rbd unmap -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Lastly you need to scale the deployment replicas back up to 1: kubectl scale deploy/zigbee2mqtt --replicas 1 -n home","title":"Accessing volumes"},{"location":"unsorted/rook-ceph-maintenance/#handling-crashes","text":"Sometimes rook-ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthy do the following... # list all the crashes ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all","title":"Handling crashes"},{"location":"unsorted/rook-ceph-maintenance/#helpful-links","text":"Common issues kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192.168.42.50:/volume1/Data /mnt/nfs kubectl get pv/(kubectl get pv \\ | grep \"tautulli\" \\ | awk -F' ' '{print $1}') -n media -o json \\ | jq -r '.spec.csi.volumeAttributes.imageName' rbd map -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data rm -rf /mnt/data/* tar xvf /mnt/nfs/backups/tautulli.tar.gz -C /mnt/data # chown -R 568:568 /mnt/data/ umount /mnt/data && \\ rbd unmap -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa ceph mgr module enable rook ceph orch set backend rook ceph dashboard set-alertmanager-api-host http://kube-prometheus-stack-alertmanager.monitoring.svc:9093 ceph dashboard set-prometheus-api-host http://kube-prometheus-stack-prometheus.monitoring.svc:9090","title":"Helpful links"},{"location":"unsorted/rook-ceph-vol-migration-draft/","text":"k scale deployment/frigate -n home --replicas 0 kubectl get pv/(kubectl get pv | grep \"frigate-config[[:space:]+]\" | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 \u00b6 kubectl get pv/(kubectl get pv | grep \"frigate-config-v1[[:space:]+]\" | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076 \u00b6 Toolbox \u00b6 Access rook direct mount kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash mkdir -p /mnt/ { old,new } rbd map -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 | xargs -I {} mount {} /mnt/old rbd map -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076 | xargs -0 -I {} sh -c 'mkfs.ext4 {}; mount {} /mnt/new' cp -rp /mnt/old/. /mnt/new chown -R 568 :568 /mnt/new umount /mnt/old umount /mnt/new rbd unmap -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 && \\ rbd unmap -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076","title":"Rook ceph vol migration draft"},{"location":"unsorted/rook-ceph-vol-migration-draft/#csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685","text":"kubectl get pv/(kubectl get pv | grep \"frigate-config-v1[[:space:]+]\" | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName'","title":"csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685"},{"location":"unsorted/rook-ceph-vol-migration-draft/#csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076","text":"","title":"csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076"},{"location":"unsorted/rook-ceph-vol-migration-draft/#toolbox","text":"Access rook direct mount kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash mkdir -p /mnt/ { old,new } rbd map -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 | xargs -I {} mount {} /mnt/old rbd map -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076 | xargs -0 -I {} sh -c 'mkfs.ext4 {}; mount {} /mnt/new' cp -rp /mnt/old/. /mnt/new chown -R 568 :568 /mnt/new umount /mnt/old umount /mnt/new rbd unmap -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 && \\ rbd unmap -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076","title":"Toolbox"},{"location":"unsorted/snmp-exporter/","text":"SNMP Exporter \u00b6 Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus Clone and build the snmp-exporter generator \u00b6 sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs Update generator.yml \u00b6 Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%) Get the Cyberpower MIB \u00b6 wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/ Generate the snmp.yml \u00b6 This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS = mibs ./generator generate","title":"SNMP Exporter"},{"location":"unsorted/snmp-exporter/#snmp-exporter","text":"Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus","title":"SNMP Exporter"},{"location":"unsorted/snmp-exporter/#clone-and-build-the-snmp-exporter-generator","text":"sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs","title":"Clone and build the snmp-exporter generator"},{"location":"unsorted/snmp-exporter/#update-generatoryml","text":"Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%)","title":"Update generator.yml"},{"location":"unsorted/snmp-exporter/#get-the-cyberpower-mib","text":"wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/","title":"Get the Cyberpower MIB"},{"location":"unsorted/snmp-exporter/#generate-the-snmpyml","text":"This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS = mibs ./generator generate","title":"Generate the snmp.yml"},{"location":"unsorted/velero/","text":"Velero \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install velero Create a backup \u00b6 Create a backup for all apps: velero backup create manually-backup-1 --from-schedule velero-daily-backup Create a backup for a single app: velero backup create jackett-test-abc \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait Delete resources \u00b6 Delete the HelmRelease : kubectl delete hr jackett-test -n testing Wait Allow the application to be redeployed and create the new resources Delete the new resources: kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config Restore \u00b6 velero restore create \\ --from-backup velero-daily-backup-20201120020022 \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Velero"},{"location":"unsorted/velero/#velero","text":"Work in progress This document is a work in progress.","title":"Velero"},{"location":"unsorted/velero/#install-the-cli-tool","text":"brew install velero","title":"Install the CLI tool"},{"location":"unsorted/velero/#create-a-backup","text":"Create a backup for all apps: velero backup create manually-backup-1 --from-schedule velero-daily-backup Create a backup for a single app: velero backup create jackett-test-abc \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Create a backup"},{"location":"unsorted/velero/#delete-resources","text":"Delete the HelmRelease : kubectl delete hr jackett-test -n testing Wait Allow the application to be redeployed and create the new resources Delete the new resources: kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config","title":"Delete resources"},{"location":"unsorted/velero/#restore","text":"velero restore create \\ --from-backup velero-daily-backup-20201120020022 \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Restore"},{"location":"unsorted/opnsense/bgp/","text":"Opnsense | BGP \u00b6 Work in progress This document is a work in progress.","title":"Opnsense | BGP"},{"location":"unsorted/opnsense/bgp/#opnsense-bgp","text":"Work in progress This document is a work in progress.","title":"Opnsense | BGP"},{"location":"unsorted/opnsense/pxe/","text":"Opnsense | PXE \u00b6 Work in progress This document is a work in progress. Setting up TFTP \u00b6 Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"Opnsense | PXE"},{"location":"unsorted/opnsense/pxe/#opnsense-pxe","text":"Work in progress This document is a work in progress.","title":"Opnsense | PXE"},{"location":"unsorted/opnsense/pxe/#setting-up-tftp","text":"Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"Setting up TFTP"}]}